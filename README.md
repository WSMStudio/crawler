# crawler

## 数据格式

详见./data/gutenberg0-4999.txt. 

格式为（相邻之间只有\t， 一本书一行）

书本详情页的url	\t author(s)（可能带生辰，是否要去除）\t	title  \t	publish_year	\t description（第二个网址无description，我默认'No description'）\t	content_url(书本内容下载链接，格式不定，优先级如下text, pdf, epub no pic, epub with pic, kindle no pic, kindle with pic, txt ASCII, HTML, RDF，剩余的稀有格式如只有音频下载链接已在./data/gutenberg_attention.txt中声明， 不同格式下载方式不同)

## 目前进展

爬了第二个网址的7000本书的基本信息，content未爬取。基本信息存储在./data/gutenberg0-4999.txt以及 同目录下gutenberg5000-9999.txt。

## 遇到困难

### content爬取有两种方式，

1. 模拟用户使用右键另存为，在弹窗中选择保存位置。需要额外插件，还未学会。
2. 已爬取content的url，如果是pdf格式或者txt格式，用浏览器打开url会在该浏览器中打开新标签页，上面的内容经过掐头去尾地剪辑可以得到完整content。

第一种方法与第二种相比，缺点是需要重新爬取网页，反爬机制就得再来一遍，第二个书本网址是每天5000个。优点是（即法二缺点是）需要内存较大，时间也比较久，还有一个大问题是大书本需要加载，目前不确定是滚轮滚到下面再重新去原网页下载加载，还是已经全部下载到本地内存中，滚轮滚到下面时浏览器进一步加载，如果是前者判断加载完成还未学会。

### 第一个网址更复杂

1. 存有cookie，比如是否包含涩情书本，比如是图片显示还是行显示。如果是访问页面.../0, ..../1, .../2，cookie会消失，
2. 点击书本会出弹窗，弹窗内有下载链接，但详情页面需要在弹窗内再点击书名才可跳转。弹窗怎么识别爬取的问题尚未解决。
3. 虽然可以用一种简单粗暴的方法用序号遍历所有书.../0, ..../1, .../2 （如果用简单粗暴的方法，不会有2中的问题）， 但根据总书本和免费书本的比例约7比1，以及每天限爬，所以最好还是在free大类内爬取。
4. 就算是free大类，里面也有不少是付费的，需要进一步筛选。
5. 可以用IP代理池，还未学会

## 计划

1. 总共要爬8W本以上，第二个网址相对好爬，大概65000本，剩余的爬第一个网址。计划在爬第二个网址期间，搞定右键下载的功能，省内存省加载时间不用重复爬。若不能搞定就用浏览器打开再截头去尾。
2. 计划先将第二个网址的6W5本书的基本信息加内容可以爬后（至少有了），再研究第一个网址。

